{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://viso.ai/deep-learning/what-are-liquid-neural-networks/\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(input_dim, reservoir_dim, output_dim, spectral_radius):\n",
    "    # initialize input weights randomly\n",
    "    reservoir_weights = np.random.randn(reservoir_dim, reservoir_dim)\n",
    "    # scale reservoir weights to achieve desired spectral radius\n",
    "    reservoir_weights *= spectral_radius / np.max(np.abs(np.linalg.eigvals(reservoir_weights)))\n",
    "    # initialize input-to-reservoir weights randomly\n",
    "    input_weights = np.random.randn(reservoir_dim, input_dim)\n",
    "    # initialize output weights to zero\n",
    "    output_weights = np.zeros((reservoir_dim, output_dim))\n",
    "    \n",
    "    return reservoir_weights, input_weights, output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lnn(input_data, labels, reservoir_weights, input_weights, output_weights, leak_rate, num_epochs):\n",
    "    num_samples = input_data.shape[0]\n",
    "    reservoir_dim = reservoir_weights.shape[0]\n",
    "    reservoir_states = np.zeros((num_samples, reservoir_dim))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(num_samples):\n",
    "            # update reservoir state\n",
    "            if i > 0:\n",
    "                reservoir_states[i, :] = (1 - leak_rate) * reservoir_states[i - 1, :]\n",
    "            reservoir_states[i, :] += leak_rate * np.tanh(\n",
    "                np.dot(reservoir_weights, reservoir_states[i, :]) + \n",
    "                np.dot(input_weights, input_data[i, :]))\n",
    "            \n",
    "        # train output weights\n",
    "        output_weights = np.dot(np.linalg.pinv(reservoir_states), labels)\n",
    "        # compute training accuracy\n",
    "        train_prediction = np.dot(reservoir_states, output_weights)\n",
    "        train_accuracy = np.mean(np.argmax(train_prediction, axis=1) == np.argmax(labels, axis=1))\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, training accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "    return output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lnn(input_data, reservoir_weights, input_weights, output_weights, leak_rate):\n",
    "    num_samples = input_data.shape[0]\n",
    "    reservoir_dim = reservoir_weights.shape[0]\n",
    "    reservoir_states = np.zeros((num_samples, reservoir_dim))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # update reservoir state\n",
    "        if i > 0:\n",
    "            reservoir_states[i, :] = (1 - leak_rate) * reservoir_states[i - 1, :]\n",
    "        reservoir_states[i, :] += leak_rate * np.tanh(\n",
    "            np.dot(reservoir_weights, reservoir_states[i, :]) + \n",
    "            np.dot(input_weights, input_data[i, :]))\n",
    "        \n",
    "    # compute predictions using output weights\n",
    "    predictions = np.dot(reservoir_states, output_weights)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set LNN hyperparameters\n",
    "input_dim = 784\n",
    "reservoir_dim = 1_000\n",
    "output_dim = 10\n",
    "leak_rate = 0.1\n",
    "spectral_radius = 0.9\n",
    "num_epochs = 10\n",
    "input_scale = 1.0 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "x_train = x_train.reshape(x_train.shape[0], input_dim) * input_scale\n",
    "x_test = x_test.reshape(x_test.shape[0], input_dim) * input_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LNN weights\n",
    "reservoir_weights, input_weights, output_weights = initialize_weights(\n",
    "    input_dim, reservoir_dim, output_dim, spectral_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LNN\n",
    "output_weights = train_lnn(\n",
    "    x_train, y_train, reservoir_weights, input_weights, output_weights, leak_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the LNN on test data\n",
    "predictions = predict_lnn(\n",
    "    x_test, reservoir_weights, input_weights, output_weights, leak_rate)\n",
    "test_accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the hyperparameters you've listed for your Liquid Neural Network (LNN), you can follow a systematic approach using a combination of search strategies. Here's how you can proceed with the given parameters and some additional ones you might consider:\n",
    "\n",
    "### Existing Hyperparameters:\n",
    "1. **input_dim**: This is typically determined by the feature size of your dataset and usually does not require tuning.\n",
    "2. **reservoir_dim**: The number of neurons in the reservoir. This can significantly affect the capacity and memory of the network.\n",
    "3. **output_dim**: Similar to input_dim, this is usually determined by the number of classes or the output size required by your task.\n",
    "4. **leak_rate**: Controls the rate at which the reservoir's internal state updates. It's crucial for the dynamics of the network.\n",
    "5. **spectral_radius**: Influences the echo state property of the reservoir. It's vital for the stability and performance of the network.\n",
    "6. **num_epochs**: The number of times the entire training dataset is passed forward and backward through the LNN.\n",
    "\n",
    "### Additional Hyperparameters to Consider:\n",
    "7. **input_scaling**: The factor by which input signals are scaled before being fed into the reservoir. This can affect how the inputs influence the reservoir dynamics.\n",
    "8. **connectivity**: The proportion of nonzero connections in the reservoir. Sparse connections often lead to more efficient and diverse dynamic reservoirs.\n",
    "9. **regularization_parameter**: Helps prevent overfitting by adding a penalty to the loss function based on the weights' magnitude.\n",
    "\n",
    "### Tuning Strategy:\n",
    "1. **Define a Performance Metric**: Choose a metric like accuracy, mean squared error, etc., depending on your specific task.\n",
    "\n",
    "2. **Data Splitting**: Split your data into training, validation, and test sets.\n",
    "\n",
    "3. **Search Strategy**:\n",
    "   - **Grid Search**: Start with a coarse grid search over a broad range of values for `reservoir_dim`, `leak_rate`, `spectral_radius`, `input_scaling`, and `connectivity`.\n",
    "   - **Random Search**: Conduct a random search within specific ranges based on the results from the grid search to find more optimal values.\n",
    "   - **Bayesian Optimization**: Use Bayesian optimization for fine-tuning the most sensitive parameters, typically `leak_rate` and `spectral_radius`.\n",
    "\n",
    "4. **Evaluation and Iteration**:\n",
    "   - For each combination of hyperparameters, train the LNN on the training set and evaluate it on the validation set using your chosen performance metric.\n",
    "   - Record the performance for each set of parameters.\n",
    "\n",
    "5. **Refinement**:\n",
    "   - Narrow down the ranges for the hyperparameters based on the results from previous steps and repeat the search if necessary.\n",
    "\n",
    "6. **Final Evaluation**:\n",
    "   - Train the LNN using the best hyperparameters from the validation phase on the combined training and validation set.\n",
    "   - Evaluate the model on the test set to gauge its performance on unseen data.\n",
    "\n",
    "7. **Analysis**:\n",
    "   - Analyze how different hyperparameters impacted the performance and stability of the LNN.\n",
    "   - Adjust the tuning strategy based on these insights if needed.\n",
    "\n",
    "By following this structured approach, you can effectively tune the hyperparameters of your Liquid Neural Network to optimize its performance for your specific task. Remember, tuning is an iterative process, and multiple rounds might be necessary to find the optimal configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
